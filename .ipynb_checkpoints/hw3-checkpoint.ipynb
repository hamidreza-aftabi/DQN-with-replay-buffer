{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 533V: Assignment 3 - Behavioral Cloning and Deep Q Learning\n",
    "\n",
    "## 48 points total (9% of final grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamidreza Aftabi 41655473\n",
    "## Amin MohammadiNasrabadi 28486827"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This assignment will help you transition from tabular approaches, topic of HW 2, to deep neural network approaches. You will implement the [Atari DQN / Deep Q-Learning](https://arxiv.org/abs/1312.5602) algorithm, which arguably kicked off the modern Deep Reinforcement Learning craze.\n",
    "\n",
    "In this assignment we will use PyTorch as our deep learning framework.  To familiarize yourself with PyTorch, your first task is to use a behavior cloning (BC) approach to learn a policy.  Behavior cloning is a supervised learning method in which there exists a dataset of expert demonstrations (state-action pairs) and the goal is to learn a policy $\\pi$ that mimics this expert.  At any given state, your policy should choose the same action the export would.\n",
    "\n",
    "Since BC avoids the need to collect data from the policy you are trying to learn, it is relatively simple. \n",
    "This makes it a nice stepping stone for implementing DQN. Furthermore, BC is relevant to modern approaches---for example its use as an initialization for systems like [AlphaGo][go] and [AlphaStar][star], which then use RL to further adapte the BC result.  \n",
    "\n",
    "<!--\n",
    "\n",
    "I feel like this might be better suited to going lower in the document:\n",
    "\n",
    "Unfortunately, in many tasks it is impossible to collect good expert demonstrations, making\n",
    "\n",
    "it's not always possible to have good expert demonstrations for a task in an environemnt and this is where reinforcement learning comes handy. Through the reward signal retrieved by interacting with the environment, the agent learns by itself what is a good policy and can learn to outperform the experts.\n",
    "\n",
    "-->\n",
    "\n",
    "Goals:\n",
    "- Famliarize yourself with PyTorch and its API including models, datasets, dataloaders\n",
    "- Implement a supervised learning approach (behavioral cloning) to learn a policy.\n",
    "- Implement the DQN objective and learn a policy through environment interaction.\n",
    "\n",
    "[go]:  https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[star]: https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii\n",
    "\n",
    "## Submission information\n",
    "\n",
    "- Complete the assignment by editing and executing the associated Python files.\n",
    "- Copy and paste the code and the terminal output requested in the predefined cells on this Jupyter notebook.\n",
    "- When done, upload the completed Jupyter notebook (ipynb file) on canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Preliminaries\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "If you have never used PyTorch before, we recommend you follow this [60 Minutes Blitz][blitz] tutorial from the official website. It should give you enough context to be able to complete the assignment.\n",
    "\n",
    "\n",
    "**If you have issues, post questions to Piazza**\n",
    "\n",
    "### Installation\n",
    "\n",
    "To install all required python packages:\n",
    "\n",
    "```\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Debugging\n",
    "\n",
    "\n",
    "You can include:  `import ipdb; ipdb.set_trace()` in your code and it will drop you to that point in the code, where you can interact with variables and test out expressions.  We recommend this as an effective method to debug the algorithms.\n",
    "\n",
    "\n",
    "[blitz]: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Behavioral Cloning\n",
    "\n",
    "Behavioral Cloning is a type of supervised learning in which you are given a dataset of expert demonstrations tuple $(s, a)$ and the goal is to learn a policy function $\\hat a = \\pi(s)$, such that $\\hat a = a$.\n",
    "\n",
    "The optimization objective is $\\min_\\theta D(\\pi(s), a)$ where $\\theta$ are the parameters the policy $\\pi$, in our case the weights of a neural network, and where $D$ represents some difference between the actions.\n",
    "\n",
    "---\n",
    "\n",
    "Before starting, we suggest reading through the provided files.\n",
    "\n",
    "For Behavioral Cloning, the important files to understand are: `model.py`, `dataset.py` and `bc.py`.\n",
    "\n",
    "- The file `model.py` has the skeleton for the model (which you will have to complete in the following questions),\n",
    "\n",
    "- The file `dataset.py` has the skeleton for the dataset the model is being trained with,\n",
    "\n",
    "- and, `bc.py` will have all the structure for training the model with the dataset.\n",
    "\n",
    "\n",
    "### 1.1 Dataset\n",
    "\n",
    "We provide a pickle file with pre-collected expert demonstrations on CartPole from which to learn the policy $\\pi$. The data has been collected from an expert policy on the environment, with the addition of a small amount of gaussian noise to the actions.\n",
    "\n",
    "The pickle file contains a list of tuples of states and actions in `numpy` in the following way:\n",
    "\n",
    "```\n",
    "[(state s, action a), (state s, action a), (state s, action a), ...]\n",
    "```\n",
    "\n",
    "In the `dataset.py` file, we provide skeleton code for creating a custom dataset. The provided code shows how to load the file.\n",
    "\n",
    "Your goal is to overwrite the `__getitem__` function in order to return a dictionary of tensors of the correct type.\n",
    "\n",
    "Hint: Look in the `bc.py` file to understand how the dataset is used.\n",
    "\n",
    "Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 2 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER TO INSERT YOUR __getitem__ method here\n",
    "def __getitem__(self, index):\n",
    "    \n",
    "        item = self.data[index]\n",
    "        state_tensor = torch.tensor(item[0]).float()\n",
    "        if item[1] == 0:\n",
    "            action_tensor= torch.tensor([1,0]).float()\n",
    "        else:\n",
    "            action_tensor= torch.tensor([0,1]).float()       \n",
    "        state_action_dic = {'state': state_tensor, 'action': action_tensor}\n",
    "        \n",
    "        return state_action_dic\n",
    "    \n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[QUESTION 2 points]** How big is the dataset provided?\n",
    "\n",
    "**YOUR ANSWER HERE** The dataset consists of 99660 tuples of states and actions.\n",
    "\n",
    "- **[QUESTION 2 points]** What is the dimensionality of $s$ and what range does each dimension of $s$ span?  I.e., how much of the state space does the expert data cover?\n",
    "\n",
    "**YOUR ANSWER HERE** Each state has four values, the same as the previous assignment: the position of the Cart, the velocity of the Cart, Pole's angle, and Pole's angular velocity. The range that each dimension of $s$ span is provided as below:\n",
    "![span.png](span.png)\n",
    "\n",
    "As it can be seen, the state space is partially covered, and the coverage is not symmetric. The possible maximum and minimum  values for each state are as follows based on the Cartpole documentation:  \n",
    "\n",
    "Cart Position: Min = -4.8, Max = 4.8. Cart Velocity: Min = -Inf, Max = Inf. Pole Angle: Min = -0.418 rad (-24 deg), Max = 0.418 rad (24 deg). Pole Angular Velocity: Min = -Inf, Max = Inf.\n",
    "\n",
    "- **[QUESTION 2 points]** What are the dimensionalities and ranges of the action $a$ in the dataset (how much of the action space does the expert data cover)?\n",
    "\n",
    "**YOUR ANSWER HERE** Although the question mentioned that the data had been collected, with the addition of a small amount of Gaussian noise to the actions $a$, the provided dataset only contains 2 discrete actions without any noise; going to the right (1) and going to the left (0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Environment\n",
    "\n",
    "Recall the state and action space of CartPole, from the previous assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[QUESTION 2 points]** Considering the full state and action spaces, do you think the provided expert dataset has good coverage?  Why or why not? How might this impact the performance of our cloned policy?\n",
    "\n",
    "**YOUR ANSWER HERE**  The action space is fully covered since it is discrete and contains only two actions, right(1) and left(0). In fact, the agent can learn and control the pole balancing only by commanding the Cart to go left or right. Regarding the states, it does not have good coverage for Cart's position and velocity, especially for negative values.  Furthermore, the coverage is not symmetric, which opposes the nature of the system.  However, in the previous assignment, we have shown that the position and velocity of the Cart are relatively less important compared to other states. Therefore, the provided coverage for Cart's position and velocity might not disrupt the learning. For the pole angle, we know that the episode termination happens when the pole angle is more than 12 degrees (0.2 rad). Again the coverage is not symmetric; It covers a relatively good domain of positive angles, but the coverage is not satisfactory for negative values. Finally, the domain covered by the pole's velocity is more symmetric compared to other states. But it could still be more comprehensive. If the expert dataset is not comprehensive enough, the agent might fail to choose the proper action in an unseen situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model\n",
    "\n",
    "The file `model.py` provides skeleton code for the model. Your goal is to create the architecture of the network by adding layers that map the input to output.\n",
    "\n",
    "You will need to update the `__init__` method and the `forward` method.\n",
    "\n",
    "The `select_action` method has already been written for you.  This should be used when running the policy in the environment, while the `forward` function should be used at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 5 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER TO INSERT YOUR MyModel class here\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, state_size, action_size):        \n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.relu(self.fc1(x))\n",
    "        y = torch.relu(self.fc2(y))\n",
    "        \n",
    "        ##### For BC we use this line of code as the output of the forward function since the output is the probability of choosing actions.\n",
    "        y = torch.sigmoid(self.fc3(y))    \n",
    "\n",
    "        ##### For DQN we use this line of code as the output of the forward function since the output is the Q(s,s). You can uncomment it if needed.\n",
    "        #y = self.fc3(y)\n",
    "        \n",
    "        \n",
    "        return y\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        self.eval()\n",
    "        z = self.forward(state)\n",
    "        self.train()\n",
    "        return z.max(1)[1].view(1, 1).to(torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "- **[QUESTION 2 points]** What is the input of the network?\n",
    "\n",
    "**YOUR ANSWER HERE** The input to the network are states. Basically, the purpose of BC/DQN is to find a policy that can choose the best action/estimate Q-values (to agian choose the best action) based on states.\n",
    "\n",
    "- **[QUESTION 2 points]** What is the output?\n",
    "\n",
    "**YOUR ANSWER HERE** As commented in the code above, the output of the network for the Behavioral Cloning section is action since we want to calculate the probability of choosing actions; thus, we have used the sigmoid function as an activation function in the last layer. However, the output of network for DQN section is Q-values because we want to estimate the Q-value for each state-action. As a result, the output of the network for the DQN section is a linear function without any activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Training\n",
    "\n",
    "The file `bc.py` is the entry point for training your behavioral cloning model. The skeleton and the main components are already there.\n",
    "\n",
    "The missing parts for you to do are:\n",
    "\n",
    "- Initializing the model\n",
    "- Choosing a loss function\n",
    "- Choosing an optimizer\n",
    "- Playing with hyperparameters to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 5 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER FOR YOUR CODE HER\n",
    "# HOW DID YOU INITIALIZE YOUR MODEL, OPTIMIZER AND LOSS FUNCTIONS? PASTE HERE YOUR FINAL CODE\n",
    "# NOTE: YOU CAN KEEP THE FOLLOWING LINES COMMENTED OUT, AS RUNNING THIS CELL WILL PROBABLY RESULT IN ERRORS\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "from eval_policy import eval_policy, device\n",
    "from model import MyModel\n",
    "from dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TOTAL_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "PRINT_INTERVAL = 500\n",
    "TEST_INTERVAL = 2\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "dataset = Dataset(data_path=\"CartPole-v0_dataset.pkl\".format(ENV_NAME))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# TODO INITIALIZE YOUR MODEL HERE\n",
    "model = MyModel(4,2)\n",
    "\n",
    "\n",
    "def train_behavioral_cloning():\n",
    "\n",
    "    # TODO CHOOSE A OPTIMIZER AND A LOSS FUNCTION FOR TRAINING YOUR NETWORK\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    loss_function =  nn.BCELoss()  \n",
    "    \n",
    "    gradient_steps = 0\n",
    "        \n",
    "    \n",
    "    for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "        for iteration, data in enumerate(dataloader):\n",
    "            data = {k: v.to(device) for k, v in data.items()}\n",
    "            output = model(data['state'])\n",
    "            loss = loss_function(output, data[\"action\"])\n",
    "          \n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "          \n",
    "            if gradient_steps % PRINT_INTERVAL == 0:\n",
    "                print('[epoch {:4d}/{}] [iter {:7d}] [loss {:.5f}]'\n",
    "                      .format(epoch, TOTAL_EPOCHS, gradient_steps, loss.item()))\n",
    "                \n",
    "            gradient_steps += 1\n",
    "    \n",
    "        if epoch % TEST_INTERVAL == 0:\n",
    "            score = eval_policy(policy=model, env=ENV_NAME)\n",
    "            print('[Test on environment] [epoch {}/{}] [score {:.2f}]'\n",
    "                  .format(epoch, TOTAL_EPOCHS, score))\n",
    "    \n",
    "    model_name = \"behavioral_cloning_{}.pt\".format(ENV_NAME)\n",
    "    print('Saving model as {}'.format(model_name))\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_behavioral_cloning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run your code by doing:\n",
    "\n",
    "```\n",
    "python3 bc.py\n",
    "```\n",
    "\n",
    "**During all of this assignment, the code in `eval_policy.py` will be your best friend.** At any time, you can test your model by giving as argument the path to the model weights and the environment name using the following command:\n",
    "\n",
    "```\n",
    "python3 eval_policy.py --model-path /path/to/model/weights --env ENV_NAME\n",
    "````"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10\n",
    "[epoch    1/100] [iter       0] [loss 0.69297]\n",
    "[epoch    1/100] [iter     500] [loss 0.52283]\n",
    "[epoch    1/100] [iter    1000] [loss 0.34441]\n",
    "[epoch    1/100] [iter    1500] [loss 0.26182]\n",
    "[epoch    2/100] [iter    2000] [loss 0.23422]\n",
    "[epoch    2/100] [iter    2500] [loss 0.19463]\n",
    "[epoch    2/100] [iter    3000] [loss 0.14118]\n",
    "[Test on environment] [epoch 2/100] [score 122.60]\n",
    "[epoch    3/100] [iter    3500] [loss 0.11729]\n",
    "[epoch    3/100] [iter    4000] [loss 0.19960]\n",
    "[epoch    3/100] [iter    4500] [loss 0.09700]\n",
    "[epoch    4/100] [iter    5000] [loss 0.18018]\n",
    "[epoch    4/100] [iter    5500] [loss 0.09258]\n",
    "[epoch    4/100] [iter    6000] [loss 0.11515]\n",
    "[Test on environment] [epoch 4/100] [score 200.00]\n",
    "[epoch    5/100] [iter    6500] [loss 0.19188]\n",
    "[epoch    5/100] [iter    7000] [loss 0.10943]\n",
    "[epoch    5/100] [iter    7500] [loss 0.12896]\n",
    "[epoch    6/100] [iter    8000] [loss 0.13201]\n",
    "[epoch    6/100] [iter    8500] [loss 0.08209]\n",
    "[epoch    6/100] [iter    9000] [loss 0.06358]\n",
    "[Test on environment] [epoch 6/100] [score 199.40]\n",
    "[epoch    7/100] [iter    9500] [loss 0.14921]\n",
    "[epoch    7/100] [iter   10000] [loss 0.09382]\n",
    "[epoch    7/100] [iter   10500] [loss 0.05394]\n",
    "[epoch    8/100] [iter   11000] [loss 0.04496]\n",
    "[epoch    8/100] [iter   11500] [loss 0.12807]\n",
    "[epoch    8/100] [iter   12000] [loss 0.04455]\n",
    "[Test on environment] [epoch 8/100] [score 200.00]\n",
    "[epoch    9/100] [iter   12500] [loss 0.10363]\n",
    "[epoch    9/100] [iter   13000] [loss 0.02825]\n",
    "[epoch    9/100] [iter   13500] [loss 0.04237]\n",
    "[epoch    9/100] [iter   14000] [loss 0.05382]\n",
    "[epoch   10/100] [iter   14500] [loss 0.06769]\n",
    "[epoch   10/100] [iter   15000] [loss 0.07356]\n",
    "[epoch   10/100] [iter   15500] [loss 0.03866]\n",
    "[Test on environment] [epoch 10/100] [score 197.00]\n",
    "[epoch   11/100] [iter   16000] [loss 0.05989]\n",
    "[epoch   11/100] [iter   16500] [loss 0.04070]\n",
    "[epoch   11/100] [iter   17000] [loss 0.01902]\n",
    "[epoch   12/100] [iter   17500] [loss 0.04678]\n",
    "[epoch   12/100] [iter   18000] [loss 0.07313]\n",
    "[epoch   12/100] [iter   18500] [loss 0.03181]\n",
    "[Test on environment] [epoch 12/100] [score 200.00]\n",
    "[epoch   13/100] [iter   19000] [loss 0.04108]\n",
    "[epoch   13/100] [iter   19500] [loss 0.02616]\n",
    "[epoch   13/100] [iter   20000] [loss 0.01922]\n",
    "[epoch   14/100] [iter   20500] [loss 0.02189]\n",
    "[epoch   14/100] [iter   21000] [loss 0.01703]\n",
    "[epoch   14/100] [iter   21500] [loss 0.06299]\n",
    "[Test on environment] [epoch 14/100] [score 199.00]\n",
    "[epoch   15/100] [iter   22000] [loss 0.01930]\n",
    "[epoch   15/100] [iter   22500] [loss 0.01844]\n",
    "[epoch   15/100] [iter   23000] [loss 0.01775]\n",
    "[epoch   16/100] [iter   23500] [loss 0.04576]\n",
    "[epoch   16/100] [iter   24000] [loss 0.00894]\n",
    "[epoch   16/100] [iter   24500] [loss 0.03795]\n",
    "[Test on environment] [epoch 16/100] [score 200.00]\n",
    "[epoch   17/100] [iter   25000] [loss 0.01835]\n",
    "[epoch   17/100] [iter   25500] [loss 0.01396]\n",
    "[epoch   17/100] [iter   26000] [loss 0.03228]\n",
    "[epoch   18/100] [iter   26500] [loss 0.02478]\n",
    "[epoch   18/100] [iter   27000] [loss 0.01912]\n",
    "[epoch   18/100] [iter   27500] [loss 0.01467]\n",
    "[epoch   18/100] [iter   28000] [loss 0.04048]\n",
    "[Test on environment] [epoch 18/100] [score 200.00]\n",
    "[epoch   19/100] [iter   28500] [loss 0.00664]\n",
    "[epoch   19/100] [iter   29000] [loss 0.01826]\n",
    "[epoch   19/100] [iter   29500] [loss 0.01443]\n",
    "[epoch   20/100] [iter   30000] [loss 0.01616]\n",
    "[epoch   20/100] [iter   30500] [loss 0.01068]\n",
    "[epoch   20/100] [iter   31000] [loss 0.02087]\n",
    "[Test on environment] [epoch 20/100] [score 199.70]\n",
    "[epoch   21/100] [iter   31500] [loss 0.02077]\n",
    "[epoch   21/100] [iter   32000] [loss 0.01944]\n",
    "[epoch   21/100] [iter   32500] [loss 0.03077]\n",
    "[epoch   22/100] [iter   33000] [loss 0.00335]\n",
    "[epoch   22/100] [iter   33500] [loss 0.00816]\n",
    "[epoch   22/100] [iter   34000] [loss 0.01649]\n",
    "[Test on environment] [epoch 22/100] [score 200.00]\n",
    "[epoch   23/100] [iter   34500] [loss 0.01541]\n",
    "[epoch   23/100] [iter   35000] [loss 0.01749]\n",
    "[epoch   23/100] [iter   35500] [loss 0.04991]\n",
    "[epoch   24/100] [iter   36000] [loss 0.01023]\n",
    "[epoch   24/100] [iter   36500] [loss 0.00693]\n",
    "[epoch   24/100] [iter   37000] [loss 0.01512]\n",
    "[Test on environment] [epoch 24/100] [score 200.00]\n",
    "[epoch   25/100] [iter   37500] [loss 0.02086]\n",
    "[epoch   25/100] [iter   38000] [loss 0.03261]\n",
    "[epoch   25/100] [iter   38500] [loss 0.00207]\n",
    "[epoch   26/100] [iter   39000] [loss 0.00637]\n",
    "[epoch   26/100] [iter   39500] [loss 0.02610]\n",
    "[epoch   26/100] [iter   40000] [loss 0.02434]\n",
    "[epoch   26/100] [iter   40500] [loss 0.02797]\n",
    "[Test on environment] [epoch 26/100] [score 198.10]\n",
    "[epoch   27/100] [iter   41000] [loss 0.01434]\n",
    "[epoch   27/100] [iter   41500] [loss 0.02505]\n",
    "[epoch   27/100] [iter   42000] [loss 0.01137]\n",
    "[epoch   28/100] [iter   42500] [loss 0.00879]\n",
    "[epoch   28/100] [iter   43000] [loss 0.00857]\n",
    "[epoch   28/100] [iter   43500] [loss 0.04055]\n",
    "[Test on environment] [epoch 28/100] [score 198.80]\n",
    "[epoch   29/100] [iter   44000] [loss 0.01600]\n",
    "[epoch   29/100] [iter   44500] [loss 0.01354]\n",
    "[epoch   29/100] [iter   45000] [loss 0.00792]\n",
    "[epoch   30/100] [iter   45500] [loss 0.00746]\n",
    "[epoch   30/100] [iter   46000] [loss 0.00435]\n",
    "[epoch   30/100] [iter   46500] [loss 0.01312]\n",
    "[Test on environment] [epoch 30/100] [score 199.60]\n",
    "[epoch   31/100] [iter   47000] [loss 0.00892]\n",
    "[epoch   31/100] [iter   47500] [loss 0.00758]\n",
    "[epoch   31/100] [iter   48000] [loss 0.00359]\n",
    "[epoch   32/100] [iter   48500] [loss 0.01483]\n",
    "[epoch   32/100] [iter   49000] [loss 0.00858]\n",
    "[epoch   32/100] [iter   49500] [loss 0.00194]\n",
    "[Test on environment] [epoch 32/100] [score 198.30]\n",
    "[epoch   33/100] [iter   50000] [loss 0.03239]\n",
    "[epoch   33/100] [iter   50500] [loss 0.00879]\n",
    "[epoch   33/100] [iter   51000] [loss 0.00968]\n",
    "[epoch   34/100] [iter   51500] [loss 0.02921]\n",
    "[epoch   34/100] [iter   52000] [loss 0.00816]\n",
    "[epoch   34/100] [iter   52500] [loss 0.00395]\n",
    "[Test on environment] [epoch 34/100] [score 199.70]\n",
    "[epoch   35/100] [iter   53000] [loss 0.00481]\n",
    "[epoch   35/100] [iter   53500] [loss 0.00780]\n",
    "[epoch   35/100] [iter   54000] [loss 0.00439]\n",
    "[epoch   35/100] [iter   54500] [loss 0.02308]\n",
    "[epoch   36/100] [iter   55000] [loss 0.01210]\n",
    "[epoch   36/100] [iter   55500] [loss 0.01702]\n",
    "[epoch   36/100] [iter   56000] [loss 0.00031]\n",
    "[Test on environment] [epoch 36/100] [score 200.00]\n",
    "[epoch   37/100] [iter   56500] [loss 0.00950]\n",
    "[epoch   37/100] [iter   57000] [loss 0.00568]\n",
    "[epoch   37/100] [iter   57500] [loss 0.00664]\n",
    "[epoch   38/100] [iter   58000] [loss 0.00095]\n",
    "[epoch   38/100] [iter   58500] [loss 0.02433]\n",
    "[epoch   38/100] [iter   59000] [loss 0.00303]\n",
    "[Test on environment] [epoch 38/100] [score 199.00]\n",
    "[epoch   39/100] [iter   59500] [loss 0.00806]\n",
    "[epoch   39/100] [iter   60000] [loss 0.01061]\n",
    "[epoch   39/100] [iter   60500] [loss 0.01774]\n",
    "[epoch   40/100] [iter   61000] [loss 0.00410]\n",
    "[epoch   40/100] [iter   61500] [loss 0.00946]\n",
    "[epoch   40/100] [iter   62000] [loss 0.02406]\n",
    "[Test on environment] [epoch 40/100] [score 200.00]\n",
    "[epoch   41/100] [iter   62500] [loss 0.01794]\n",
    "[epoch   41/100] [iter   63000] [loss 0.02325]\n",
    "[epoch   41/100] [iter   63500] [loss 0.00000]\n",
    "[epoch   42/100] [iter   64000] [loss 0.00425]\n",
    "[epoch   42/100] [iter   64500] [loss 0.01497]\n",
    "[epoch   42/100] [iter   65000] [loss 0.00529]\n",
    "[Test on environment] [epoch 42/100] [score 200.00]\n",
    "[epoch   43/100] [iter   65500] [loss 0.00389]\n",
    "[epoch   43/100] [iter   66000] [loss 0.02011]\n",
    "[epoch   43/100] [iter   66500] [loss 0.00624]\n",
    "[epoch   44/100] [iter   67000] [loss 0.00323]\n",
    "[epoch   44/100] [iter   67500] [loss 0.02066]\n",
    "[epoch   44/100] [iter   68000] [loss 0.00001]\n",
    "[epoch   44/100] [iter   68500] [loss 0.00522]\n",
    "[Test on environment] [epoch 44/100] [score 199.40]\n",
    "[epoch   45/100] [iter   69000] [loss 0.00830]\n",
    "[epoch   45/100] [iter   69500] [loss 0.01097]\n",
    "[epoch   45/100] [iter   70000] [loss 0.00571]\n",
    "[epoch   46/100] [iter   70500] [loss 0.00373]\n",
    "[epoch   46/100] [iter   71000] [loss 0.00852]\n",
    "[epoch   46/100] [iter   71500] [loss 0.00051]\n",
    "[Test on environment] [epoch 46/100] [score 199.20]\n",
    "[epoch   47/100] [iter   72000] [loss 0.00980]\n",
    "[epoch   47/100] [iter   72500] [loss 0.01337]\n",
    "[epoch   47/100] [iter   73000] [loss 0.00419]\n",
    "[epoch   48/100] [iter   73500] [loss 0.00710]\n",
    "[epoch   48/100] [iter   74000] [loss 0.01321]\n",
    "[epoch   48/100] [iter   74500] [loss 0.00305]\n",
    "[Test on environment] [epoch 48/100] [score 200.00]\n",
    "[epoch   49/100] [iter   75000] [loss 0.00270]\n",
    "[epoch   49/100] [iter   75500] [loss 0.01002]\n",
    "[epoch   49/100] [iter   76000] [loss 0.01569]\n",
    "[epoch   50/100] [iter   76500] [loss 0.00179]\n",
    "[epoch   50/100] [iter   77000] [loss 0.00619]\n",
    "[epoch   50/100] [iter   77500] [loss 0.01004]\n",
    "[Test on environment] [epoch 50/100] [score 200.00]\n",
    "[epoch   51/100] [iter   78000] [loss 0.00150]\n",
    "[epoch   51/100] [iter   78500] [loss 0.00142]\n",
    "[epoch   51/100] [iter   79000] [loss 0.02386]\n",
    "[epoch   52/100] [iter   79500] [loss 0.00105]\n",
    "[epoch   52/100] [iter   80000] [loss 0.00028]\n",
    "[epoch   52/100] [iter   80500] [loss 0.00067]\n",
    "[epoch   52/100] [iter   81000] [loss 0.00061]\n",
    "[Test on environment] [epoch 52/100] [score 199.50]\n",
    "[epoch   53/100] [iter   81500] [loss 0.00540]\n",
    "[epoch   53/100] [iter   82000] [loss 0.00190]\n",
    "[epoch   53/100] [iter   82500] [loss 0.01797]\n",
    "[epoch   54/100] [iter   83000] [loss 0.01155]\n",
    "[epoch   54/100] [iter   83500] [loss 0.00104]\n",
    "[epoch   54/100] [iter   84000] [loss 0.01321]\n",
    "[Test on environment] [epoch 54/100] [score 199.30]\n",
    "[epoch   55/100] [iter   84500] [loss 0.01171]\n",
    "[epoch   55/100] [iter   85000] [loss 0.00058]\n",
    "[epoch   55/100] [iter   85500] [loss 0.00324]\n",
    "[epoch   56/100] [iter   86000] [loss 0.02099]\n",
    "[epoch   56/100] [iter   86500] [loss 0.01111]\n",
    "[epoch   56/100] [iter   87000] [loss 0.00223]\n",
    "[Test on environment] [epoch 56/100] [score 200.00]\n",
    "[epoch   57/100] [iter   87500] [loss 0.01261]\n",
    "[epoch   57/100] [iter   88000] [loss 0.00160]\n",
    "[epoch   57/100] [iter   88500] [loss 0.00176]\n",
    "[epoch   58/100] [iter   89000] [loss 0.00709]\n",
    "[epoch   58/100] [iter   89500] [loss 0.00454]\n",
    "[epoch   58/100] [iter   90000] [loss 0.00307]\n",
    "[Test on environment] [epoch 58/100] [score 200.00]\n",
    "[epoch   59/100] [iter   90500] [loss 0.00726]\n",
    "[epoch   59/100] [iter   91000] [loss 0.00420]\n",
    "[epoch   59/100] [iter   91500] [loss 0.00705]\n",
    "[epoch   60/100] [iter   92000] [loss 0.00036]\n",
    "[epoch   60/100] [iter   92500] [loss 0.00763]\n",
    "[epoch   60/100] [iter   93000] [loss 0.00108]\n",
    "[Test on environment] [epoch 60/100] [score 200.00]\n",
    "[epoch   61/100] [iter   93500] [loss 0.00155]\n",
    "[epoch   61/100] [iter   94000] [loss 0.01235]\n",
    "[epoch   61/100] [iter   94500] [loss 0.00051]\n",
    "[epoch   61/100] [iter   95000] [loss 0.00296]\n",
    "[epoch   62/100] [iter   95500] [loss 0.00498]\n",
    "[epoch   62/100] [iter   96000] [loss 0.01181]\n",
    "[epoch   62/100] [iter   96500] [loss 0.00670]\n",
    "[Test on environment] [epoch 62/100] [score 197.60]\n",
    "[epoch   63/100] [iter   97000] [loss 0.01143]\n",
    "[epoch   63/100] [iter   97500] [loss 0.00325]\n",
    "[epoch   63/100] [iter   98000] [loss 0.00009]\n",
    "[epoch   64/100] [iter   98500] [loss 0.00073]\n",
    "[epoch   64/100] [iter   99000] [loss 0.00443]\n",
    "[epoch   64/100] [iter   99500] [loss 0.01088]\n",
    "[Test on environment] [epoch 64/100] [score 199.70]\n",
    "[epoch   65/100] [iter  100000] [loss 0.00019]\n",
    "[epoch   65/100] [iter  100500] [loss 0.01080]\n",
    "[epoch   65/100] [iter  101000] [loss 0.00902]\n",
    "[epoch   66/100] [iter  101500] [loss 0.00651]\n",
    "[epoch   66/100] [iter  102000] [loss 0.01342]\n",
    "[epoch   66/100] [iter  102500] [loss 0.00270]\n",
    "[Test on environment] [epoch 66/100] [score 198.50]\n",
    "[epoch   67/100] [iter  103000] [loss 0.01002]\n",
    "[epoch   67/100] [iter  103500] [loss 0.00211]\n",
    "[epoch   67/100] [iter  104000] [loss 0.01420]\n",
    "[epoch   68/100] [iter  104500] [loss 0.00044]\n",
    "[epoch   68/100] [iter  105000] [loss 0.01844]\n",
    "[epoch   68/100] [iter  105500] [loss 0.01072]\n",
    "[Test on environment] [epoch 68/100] [score 198.90]\n",
    "[epoch   69/100] [iter  106000] [loss 0.00318]\n",
    "[epoch   69/100] [iter  106500] [loss 0.00213]\n",
    "[epoch   69/100] [iter  107000] [loss 0.00646]\n",
    "[epoch   69/100] [iter  107500] [loss 0.00374]\n",
    "[epoch   70/100] [iter  108000] [loss 0.02011]\n",
    "[epoch   70/100] [iter  108500] [loss 0.00149]\n",
    "[epoch   70/100] [iter  109000] [loss 0.01112]\n",
    "[Test on environment] [epoch 70/100] [score 200.00]\n",
    "[epoch   71/100] [iter  109500] [loss 0.00665]\n",
    "[epoch   71/100] [iter  110000] [loss 0.00714]\n",
    "[epoch   71/100] [iter  110500] [loss 0.01021]\n",
    "[epoch   72/100] [iter  111000] [loss 0.00102]\n",
    "[epoch   72/100] [iter  111500] [loss 0.00597]\n",
    "[epoch   72/100] [iter  112000] [loss 0.00702]\n",
    "[Test on environment] [epoch 72/100] [score 200.00]\n",
    "[epoch   73/100] [iter  112500] [loss 0.00773]\n",
    "[epoch   73/100] [iter  113000] [loss 0.02033]\n",
    "[epoch   73/100] [iter  113500] [loss 0.00796]\n",
    "[epoch   74/100] [iter  114000] [loss 0.00071]\n",
    "[epoch   74/100] [iter  114500] [loss 0.00410]\n",
    "[epoch   74/100] [iter  115000] [loss 0.00851]\n",
    "[Test on environment] [epoch 74/100] [score 199.30]\n",
    "[epoch   75/100] [iter  115500] [loss 0.00004]\n",
    "[epoch   75/100] [iter  116000] [loss 0.00831]\n",
    "[epoch   75/100] [iter  116500] [loss 0.00309]\n",
    "[epoch   76/100] [iter  117000] [loss 0.00016]\n",
    "[epoch   76/100] [iter  117500] [loss 0.01076]\n",
    "[epoch   76/100] [iter  118000] [loss 0.00144]\n",
    "[Test on environment] [epoch 76/100] [score 200.00]\n",
    "[epoch   77/100] [iter  118500] [loss 0.00283]\n",
    "[epoch   77/100] [iter  119000] [loss 0.00977]\n",
    "[epoch   77/100] [iter  119500] [loss 0.00083]\n",
    "[epoch   78/100] [iter  120000] [loss 0.00122]\n",
    "[epoch   78/100] [iter  120500] [loss 0.01017]\n",
    "[epoch   78/100] [iter  121000] [loss 0.00162]\n",
    "[epoch   78/100] [iter  121500] [loss 0.01329]\n",
    "[Test on environment] [epoch 78/100] [score 197.80]\n",
    "[epoch   79/100] [iter  122000] [loss 0.00911]\n",
    "[epoch   79/100] [iter  122500] [loss 0.02278]\n",
    "[epoch   79/100] [iter  123000] [loss 0.02190]\n",
    "[epoch   80/100] [iter  123500] [loss 0.00917]\n",
    "[epoch   80/100] [iter  124000] [loss 0.00010]\n",
    "[epoch   80/100] [iter  124500] [loss 0.00098]\n",
    "[Test on environment] [epoch 80/100] [score 200.00]\n",
    "[epoch   81/100] [iter  125000] [loss 0.00116]\n",
    "[epoch   81/100] [iter  125500] [loss 0.00003]\n",
    "[epoch   81/100] [iter  126000] [loss 0.00838]\n",
    "[epoch   82/100] [iter  126500] [loss 0.00124]\n",
    "[epoch   82/100] [iter  127000] [loss 0.01185]\n",
    "[epoch   82/100] [iter  127500] [loss 0.01469]\n",
    "[Test on environment] [epoch 82/100] [score 199.80]\n",
    "[epoch   83/100] [iter  128000] [loss 0.00167]\n",
    "[epoch   83/100] [iter  128500] [loss 0.00089]\n",
    "[epoch   83/100] [iter  129000] [loss 0.00163]\n",
    "[epoch   84/100] [iter  129500] [loss 0.00689]\n",
    "[epoch   84/100] [iter  130000] [loss 0.02034]\n",
    "[epoch   84/100] [iter  130500] [loss 0.00279]\n",
    "[Test on environment] [epoch 84/100] [score 200.00]\n",
    "[epoch   85/100] [iter  131000] [loss 0.00091]\n",
    "[epoch   85/100] [iter  131500] [loss 0.00350]\n",
    "[epoch   85/100] [iter  132000] [loss 0.01029]\n",
    "[epoch   86/100] [iter  132500] [loss 0.00031]\n",
    "[epoch   86/100] [iter  133000] [loss 0.00323]\n",
    "[epoch   86/100] [iter  133500] [loss 0.00781]\n",
    "[Test on environment] [epoch 86/100] [score 200.00]\n",
    "[epoch   87/100] [iter  134000] [loss 0.00977]\n",
    "[epoch   87/100] [iter  134500] [loss 0.01478]\n",
    "[epoch   87/100] [iter  135000] [loss 0.00776]\n",
    "[epoch   87/100] [iter  135500] [loss 0.00170]\n",
    "[epoch   88/100] [iter  136000] [loss 0.00268]\n",
    "[epoch   88/100] [iter  136500] [loss 0.00033]\n",
    "[epoch   88/100] [iter  137000] [loss 0.00053]\n",
    "[Test on environment] [epoch 88/100] [score 199.50]\n",
    "[epoch   89/100] [iter  137500] [loss 0.00052]\n",
    "[epoch   89/100] [iter  138000] [loss 0.00877]\n",
    "[epoch   89/100] [iter  138500] [loss 0.00059]\n",
    "[epoch   90/100] [iter  139000] [loss 0.02041]\n",
    "[epoch   90/100] [iter  139500] [loss 0.00540]\n",
    "[epoch   90/100] [iter  140000] [loss 0.01349]\n",
    "[Test on environment] [epoch 90/100] [score 199.70]\n",
    "[epoch   91/100] [iter  140500] [loss 0.00378]\n",
    "[epoch   91/100] [iter  141000] [loss 0.00678]\n",
    "[epoch   91/100] [iter  141500] [loss 0.00875]\n",
    "[epoch   92/100] [iter  142000] [loss 0.00023]\n",
    "[epoch   92/100] [iter  142500] [loss 0.00363]\n",
    "[epoch   92/100] [iter  143000] [loss 0.00258]\n",
    "[Test on environment] [epoch 92/100] [score 200.00]\n",
    "[epoch   93/100] [iter  143500] [loss 0.00156]\n",
    "[epoch   93/100] [iter  144000] [loss 0.00220]\n",
    "[epoch   93/100] [iter  144500] [loss 0.01874]\n",
    "[epoch   94/100] [iter  145000] [loss 0.00053]\n",
    "[epoch   94/100] [iter  145500] [loss 0.00002]\n",
    "[epoch   94/100] [iter  146000] [loss 0.00157]\n",
    "[Test on environment] [epoch 94/100] [score 200.00]\n",
    "[epoch   95/100] [iter  146500] [loss 0.00113]\n",
    "[epoch   95/100] [iter  147000] [loss 0.00004]\n",
    "[epoch   95/100] [iter  147500] [loss 0.00021]\n",
    "[epoch   95/100] [iter  148000] [loss 0.00276]\n",
    "[epoch   96/100] [iter  148500] [loss 0.00006]\n",
    "[epoch   96/100] [iter  149000] [loss 0.02263]\n",
    "[epoch   96/100] [iter  149500] [loss 0.03030]\n",
    "[Test on environment] [epoch 96/100] [score 200.00]\n",
    "[epoch   97/100] [iter  150000] [loss 0.00068]\n",
    "[epoch   97/100] [iter  150500] [loss 0.00006]\n",
    "[epoch   97/100] [iter  151000] [loss 0.00109]\n",
    "[epoch   98/100] [iter  151500] [loss 0.00558]\n",
    "[epoch   98/100] [iter  152000] [loss 0.00665]\n",
    "[epoch   98/100] [iter  152500] [loss 0.00053]\n",
    "[Test on environment] [epoch 98/100] [score 200.00]\n",
    "[epoch   99/100] [iter  153000] [loss 0.01541]\n",
    "[epoch   99/100] [iter  153500] [loss 0.00128]\n",
    "[epoch   99/100] [iter  154000] [loss 0.02509]\n",
    "[epoch  100/100] [iter  154500] [loss 0.00001]\n",
    "[epoch  100/100] [iter  155000] [loss 0.00178]\n",
    "[epoch  100/100] [iter  155500] [loss 0.00879]\n",
    "[Test on environment] [epoch 100/100] [score 198.20]\n",
    "Saving model as behavioral_cloning_CartPole-v0.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[QUESTION 2 points]** Did you manage to learn a good policy? How consistent is the reward you are getting?\n",
    "\n",
    "**YOUR ANSWER HERE** As it can be seen, although the dataset provided does not cover the full domain of the states, the agent can learn very fast and achieve reward=200 after only four epochs. However, as discussed in the next section, \"we are not always lucky enough to have access to a dataset of expert demonstrations. Replicating an expert policy suffers from compounding error. The policy $\\pi$ only sees these \"perfect\" examples and has no knowledge on how to recover from states not visited by the expert. For this reason, as soon as it is presented with a state that is off the expert trajectory, it will perform poorly and will continue to deviate from a good trajectory without the possibility of recovering from errors.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Deep Q Learning\n",
    "\n",
    "There are two main issues with the behavior cloning approach.\n",
    "\n",
    "- First, we are not always lucky enough to have access to a dataset of expert demonstrations.\n",
    "- Second, replicating an expert policy suffers from compounding error. The policy $\\pi$ only sees these \"perfect\" examples and has no knowledge on how to recover from states not visited by the expert. For this reason, as soon as it is presented with a state that is off the expert trajectory, it will perform poorly and will continue to deviate from a good trajectory without the possibility of recovering from errors.\n",
    "\n",
    "---\n",
    "The second task consists in solving the environment from scratch, using RL, and most specifically the DQN algorithm, to learn a policy $\\pi$.\n",
    "\n",
    "For this task, familiarize yourself with the file `dqn.py`. We are going to re-use the file `model.py` for the model you created in the previous task.\n",
    "\n",
    "Your task is very similar to the one in the previous assignment, to implement the Q-learning algorithm, but in this version, our Q-function is approximated with a neural network.\n",
    "\n",
    "The algorithm (excerpted from Section 6.5 of [Sutton's book](http://incompleteideas.net/book/RLbook2018.pdf)) is given below:\n",
    "\n",
    "![DQN algorithm](https://i.imgur.com/Mh4Uxta.png)\n",
    "\n",
    "### 2.0 Think about your model...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[QUESTION 2 points]** In DQN, we are using the same model as in task 1 for behavioral cloning. In both tasks the model receives as input the state and in both tasks the model outputs something that has the same dimensionality as the number of actions. These two outputs, though, represent very different things. What is each one representing?\n",
    "\n",
    "**YOUR ANSWER HERE** As discussed in the BC section, the output of the Behavioral Cloning network is the probability of choosing action. This is why we used sigmoid as the activation function in the last layer of the network. However, in the DQN network, the output is Q-values; thus, we used the linear function without any activation function in the DQN network to achieve a better result since it is regarded as the regression problem and Q values could be possibly more than [0-1] domian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Update your Q-function\n",
    "\n",
    "Complete the `optimize_model` function. This function receives as input a `state`, an `action`, the `next_state`, the `reward` and `done` representing the tuple $(s_t, a_t, s_{t+1}, r_t, done_t)$. Your task is to update your Q-function as shown in the [Atari DQN paper](https://arxiv.org/abs/1312.5602) environment. For now don't be concerned with the experience replay buffer. We'll get to that later.\n",
    "\n",
    "![Loss function](https://i.imgur.com/tpTsV8m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 8 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER TO INSERT YOUR optimize_model function here:\n",
    "\n",
    "def optimize_model(state, action, next_state, reward, done):\n",
    "    \n",
    "    # TODO given a tuple (s_t, a_t, s_{t+1}, r_t, done_t) update your model weights\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_next= target(next_state).float()\n",
    "       \n",
    "    \n",
    "    out, inds = torch.max(q_next,dim=1)\n",
    "    status  = torch.logical_not(done)+0\n",
    "    y = (reward + GAMMA * status * out).view(-1,1)\n",
    "    q = model(state)\n",
    "    Q = torch.zeros(BATCH_SIZE,1)\n",
    "    \n",
    "    \n",
    "    ##### BATCH_SIZE is BATCH_SIZE = 1 for the first part of the question, and BATCH_SIZE = 256 for the replay buffer section.\n",
    "    for i in range(BATCH_SIZE):\n",
    "        Q[i] = q[i,action[i].type(torch.LongTensor)] \n",
    "        \n",
    "        \n",
    "    loss = loss_func(y,Q)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 $\\epsilon$-greedy strategy\n",
    "\n",
    "You will need a strategy to explore your environment. The standard strategy is to use $\\epsilon$-greedy. Implement it in the `choose_action` function template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 5 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER TO INSERT YOUR choose_action function here:\n",
    "\n",
    "def choose_action(state, i_episode, test_mode=False):\n",
    "    # TODO implement an epsilon-greedy strategy\n",
    "    \n",
    "    # We used Linear Annealing for decaying epsilon to reach faster convergence\n",
    "    epsilon = max(0.02, 0.1 - 0.01*(i_episode/200)) \n",
    "\n",
    "    ee_tradeoff = np.random.random()\n",
    "\n",
    "    if ee_tradeoff <= epsilon:\n",
    "        action = torch.tensor(env.action_space.sample())  \n",
    "    else:\n",
    "        action = torch.argmax(model(torch.tensor(state).float()))\n",
    "    \n",
    "    return action.view(-1,1)\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train your model\n",
    "\n",
    "Try to train a model in this way.\n",
    "\n",
    "You can run your code by doing:\n",
    "\n",
    "```\n",
    "python3 dqn.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[QUESTION 2 points]** How many episodes does it take to learn (ie. reach a good reward)?\n",
    "\n",
    "**YOUR ANSWER HERE** It has reached the maximum reward in a relatively stable situation after almost 1450 episodes. In some epochs, we see noisy behavior. This actually happens due to the stochastic nature of our training procedure. We will see in the next section that we can achieve faster convergence and more stable results using buffer replay. We have also used decaying epsilon, which will help have a more stable result in a shorter time."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[Episode   10/2000] [Steps   12] [reward 13.0]\n",
    "[Episode   20/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 25] [Average Reward 9.5]\n",
    "----------\n",
    "[Episode   30/2000] [Steps    9] [reward 10.0]\n",
    "[Episode   40/2000] [Steps   11] [reward 12.0]\n",
    "[Episode   50/2000] [Steps   11] [reward 12.0]\n",
    "----------\n",
    "[TEST Episode 50] [Average Reward 9.5]\n",
    "----------\n",
    "[Episode   60/2000] [Steps    8] [reward 9.0]\n",
    "[Episode   70/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 75] [Average Reward 9.3]\n",
    "----------\n",
    "[Episode   80/2000] [Steps    9] [reward 10.0]\n",
    "[Episode   90/2000] [Steps   11] [reward 12.0]\n",
    "[Episode  100/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 100] [Average Reward 9.8]\n",
    "----------\n",
    "[Episode  110/2000] [Steps   12] [reward 13.0]\n",
    "[Episode  120/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 125] [Average Reward 9.4]\n",
    "----------\n",
    "[Episode  130/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  140/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  150/2000] [Steps    7] [reward 8.0]\n",
    "----------\n",
    "[TEST Episode 150] [Average Reward 9.7]\n",
    "----------\n",
    "[Episode  160/2000] [Steps    7] [reward 8.0]\n",
    "[Episode  170/2000] [Steps   10] [reward 11.0]\n",
    "----------\n",
    "[TEST Episode 175] [Average Reward 9.8]\n",
    "----------\n",
    "[Episode  180/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  190/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  200/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 200] [Average Reward 9.0]\n",
    "----------\n",
    "[Episode  210/2000] [Steps   11] [reward 12.0]\n",
    "[Episode  220/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 225] [Average Reward 9.3]\n",
    "----------\n",
    "[Episode  230/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  240/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  250/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 250] [Average Reward 9.7]\n",
    "----------\n",
    "[Episode  260/2000] [Steps   11] [reward 12.0]\n",
    "[Episode  270/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 275] [Average Reward 9.2]\n",
    "----------\n",
    "[Episode  280/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  290/2000] [Steps   10] [reward 11.0]\n",
    "[Episode  300/2000] [Steps   10] [reward 11.0]\n",
    "----------\n",
    "[TEST Episode 300] [Average Reward 9.7]\n",
    "----------\n",
    "[Episode  310/2000] [Steps    7] [reward 8.0]\n",
    "[Episode  320/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 325] [Average Reward 9.3]\n",
    "----------\n",
    "[Episode  330/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  340/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  350/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 350] [Average Reward 9.5]\n",
    "----------\n",
    "[Episode  360/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  370/2000] [Steps    7] [reward 8.0]\n",
    "----------\n",
    "[TEST Episode 375] [Average Reward 9.1]\n",
    "----------\n",
    "[Episode  380/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  390/2000] [Steps   11] [reward 12.0]\n",
    "[Episode  400/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 400] [Average Reward 9.2]\n",
    "----------\n",
    "[Episode  410/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  420/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 425] [Average Reward 9.4]\n",
    "----------\n",
    "[Episode  430/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  440/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  450/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 450] [Average Reward 9.5]\n",
    "----------\n",
    "[Episode  460/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  470/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 475] [Average Reward 9.3]\n",
    "----------\n",
    "[Episode  480/2000] [Steps    7] [reward 8.0]\n",
    "[Episode  490/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  500/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 500] [Average Reward 9.4]\n",
    "----------\n",
    "[Episode  510/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  520/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 525] [Average Reward 9.1]\n",
    "----------\n",
    "[Episode  530/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  540/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  550/2000] [Steps   10] [reward 11.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 550] [Average Reward 10.3]\n",
    "----------\n",
    "[Episode  560/2000] [Steps   11] [reward 12.0]\n",
    "[Episode  570/2000] [Steps   49] [reward 50.0]\n",
    "----------\n",
    "[TEST Episode 575] [Average Reward 10.0]\n",
    "----------\n",
    "[Episode  580/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  590/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  600/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 600] [Average Reward 9.2]\n",
    "----------\n",
    "[Episode  610/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  620/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 625] [Average Reward 9.7]\n",
    "----------\n",
    "[Episode  630/2000] [Steps   10] [reward 11.0]\n",
    "[Episode  640/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  650/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 650] [Average Reward 9.6]\n",
    "----------\n",
    "[Episode  660/2000] [Steps   13] [reward 14.0]\n",
    "[Episode  670/2000] [Steps  149] [reward 150.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 675] [Average Reward 64.2]\n",
    "----------\n",
    "[Episode  680/2000] [Steps   50] [reward 51.0]\n",
    "[Episode  690/2000] [Steps   76] [reward 77.0]\n",
    "[Episode  700/2000] [Steps   67] [reward 68.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 700] [Average Reward 65.4]\n",
    "----------\n",
    "[Episode  710/2000] [Steps   64] [reward 65.0]\n",
    "[Episode  720/2000] [Steps   79] [reward 80.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 725] [Average Reward 83.6]\n",
    "----------\n",
    "[Episode  730/2000] [Steps   30] [reward 31.0]\n",
    "[Episode  740/2000] [Steps   57] [reward 58.0]\n",
    "[Episode  750/2000] [Steps   29] [reward 30.0]\n",
    "----------\n",
    "[TEST Episode 750] [Average Reward 48.8]\n",
    "----------\n",
    "[Episode  760/2000] [Steps   64] [reward 65.0]\n",
    "[Episode  770/2000] [Steps  141] [reward 142.0]\n",
    "----------\n",
    "[TEST Episode 775] [Average Reward 55.4]\n",
    "----------\n",
    "[Episode  780/2000] [Steps   79] [reward 80.0]\n",
    "[Episode  790/2000] [Steps  121] [reward 122.0]\n",
    "[Episode  800/2000] [Steps   74] [reward 75.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 800] [Average Reward 89.8]\n",
    "----------\n",
    "[Episode  810/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  820/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 825] [Average Reward 9.4]\n",
    "----------\n",
    "[Episode  830/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  840/2000] [Steps   11] [reward 12.0]\n",
    "[Episode  850/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 850] [Average Reward 9.1]\n",
    "----------\n",
    "[Episode  860/2000] [Steps    7] [reward 8.0]\n",
    "[Episode  870/2000] [Steps    7] [reward 8.0]\n",
    "----------\n",
    "[TEST Episode 875] [Average Reward 9.1]\n",
    "----------\n",
    "[Episode  880/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  890/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  900/2000] [Steps   10] [reward 11.0]\n",
    "----------\n",
    "[TEST Episode 900] [Average Reward 9.5]\n",
    "----------\n",
    "[Episode  910/2000] [Steps   11] [reward 12.0]\n",
    "[Episode  920/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 925] [Average Reward 9.6]\n",
    "----------\n",
    "[Episode  930/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  940/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  950/2000] [Steps   10] [reward 11.0]\n",
    "----------\n",
    "[TEST Episode 950] [Average Reward 11.2]\n",
    "----------\n",
    "[Episode  960/2000] [Steps   62] [reward 63.0]\n",
    "[Episode  970/2000] [Steps   75] [reward 76.0]\n",
    "----------\n",
    "[TEST Episode 975] [Average Reward 48.0]\n",
    "----------\n",
    "[Episode  980/2000] [Steps   85] [reward 86.0]\n",
    "[Episode  990/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1000/2000] [Steps   81] [reward 82.0]\n",
    "----------\n",
    "[TEST Episode 1000] [Average Reward 58.7]\n",
    "----------\n",
    "[Episode 1010/2000] [Steps  114] [reward 115.0]\n",
    "[Episode 1020/2000] [Steps   53] [reward 54.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 1025] [Average Reward 90.3]\n",
    "----------\n",
    "[Episode 1030/2000] [Steps  158] [reward 159.0]\n",
    "[Episode 1040/2000] [Steps  172] [reward 173.0]\n",
    "[Episode 1050/2000] [Steps  162] [reward 163.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 1050] [Average Reward 158.4]\n",
    "----------\n",
    "[Episode 1060/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1070/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1075] [Average Reward 102.7]\n",
    "----------\n",
    "[Episode 1080/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1090/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1100/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 1100] [Average Reward 159.6]\n",
    "----------\n",
    "[Episode 1110/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1120/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1125] [Average Reward 132.8]\n",
    "----------\n",
    "[Episode 1130/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1140/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1150/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1150] [Average Reward 128.5]\n",
    "----------\n",
    "[Episode 1160/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1170/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1175] [Average Reward 37.3]\n",
    "----------\n",
    "[Episode 1180/2000] [Steps  195] [reward 196.0]\n",
    "[Episode 1190/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1200/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1200] [Average Reward 114.2]\n",
    "----------\n",
    "[Episode 1210/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1220/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1225] [Average Reward 144.9]\n",
    "----------\n",
    "[Episode 1230/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1240/2000] [Steps  180] [reward 181.0]\n",
    "[Episode 1250/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 1250] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1260/2000] [Steps  174] [reward 175.0]\n",
    "[Episode 1270/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1275] [Average Reward 142.9]\n",
    "----------\n",
    "[Episode 1280/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1290/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1300/2000] [Steps  159] [reward 160.0]\n",
    "----------\n",
    "[TEST Episode 1300] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1310/2000] [Steps  162] [reward 163.0]\n",
    "[Episode 1320/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1325] [Average Reward 71.9]\n",
    "----------\n",
    "[Episode 1330/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1340/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1350/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1350] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1360/2000] [Steps    7] [reward 8.0]\n",
    "[Episode 1370/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 1375] [Average Reward 9.4]\n",
    "----------\n",
    "[Episode 1380/2000] [Steps    9] [reward 10.0]\n",
    "[Episode 1390/2000] [Steps    8] [reward 9.0]\n",
    "[Episode 1400/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 1400] [Average Reward 9.6]\n",
    "----------\n",
    "[Episode 1410/2000] [Steps    8] [reward 9.0]\n",
    "[Episode 1420/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 1425] [Average Reward 9.4]\n",
    "----------\n",
    "[Episode 1430/2000] [Steps    9] [reward 10.0]\n",
    "[Episode 1440/2000] [Steps    8] [reward 9.0]\n",
    "[Episode 1450/2000] [Steps    9] [reward 10.0]\n",
    "----------\n",
    "[TEST Episode 1450] [Average Reward 9.3]\n",
    "----------\n",
    "[Episode 1460/2000] [Steps    180] [reward 181.0]\n",
    "[Episode 1470/2000] [Steps    173] [reward 174.0]\n",
    "----------\n",
    "[TEST Episode 1475] [Average Reward 9.5]\n",
    "----------\n",
    "[Episode 1480/2000] [Steps    190] [reward 191.0]\n",
    "[Episode 1490/2000] [Steps    187] [reward 188.0]\n",
    "[Episode 1500/2000] [Steps    199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1500] [Average Reward 9.6]\n",
    "----------\n",
    "[Episode 1510/2000] [Steps    181] [reward 182.0]\n",
    "[Episode 1520/2000] [Steps    193] [reward 194.0]\n",
    "----------\n",
    "[TEST Episode 1525] [Average Reward 17.7]\n",
    "----------\n",
    "[Episode 1530/2000] [Steps   194] [reward 195.0]\n",
    "[Episode 1540/2000] [Steps   183] [reward 184.0]\n",
    "[Episode 1550/2000] [Steps   199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1550] [Average Reward 122.5]\n",
    "----------\n",
    "[Episode 1560/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1570/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1575] [Average Reward 115.7]\n",
    "----------\n",
    "[Episode 1580/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1590/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1600/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1600] [Average Reward 135.3]\n",
    "----------\n",
    "[Episode 1610/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1620/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1625] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1630/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1640/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1650/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1650] [Average Reward 171.2]\n",
    "----------\n",
    "[Episode 1660/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1670/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1675] [Average Reward 171.1]\n",
    "----------\n",
    "[Episode 1680/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1690/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1700/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1700] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1710/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1720/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1725] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1730/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1740/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1750/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1750] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1760/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1770/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1775] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1780/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1790/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1800/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1800] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1810/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1820/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1825] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1830/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1840/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1850/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1850] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1860/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1870/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1875] [Average Reward 198.4]\n",
    "----------\n",
    "[Episode 1880/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1890/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1900/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1900] [Average Reward 198.3]\n",
    "----------\n",
    "[Episode 1910/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1920/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1925] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1930/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1940/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1950/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1950] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1960/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1970/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1975] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1980/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1990/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 2000/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 2000] [Average Reward 200.0]\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Add the Experience Replay Buffer\n",
    "\n",
    "If you read the DQN paper (and as you can see from the algorithm picture above), the authors make use of an experience replay buffer to learn faster. We provide an implementation in the file `replay_buffer.py`. Update the `train_reinforcement_learning` code to push a tuple to the replay buffer and to sample a batch for the `optimize_model` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[QUESTION 5 points]** How does the replay buffer improve performances?\n",
    "\n",
    "**YOUR ANSWER HERE** It is obvious that by using Experience Replay Buffer, the agent could reach the maximum reward in stable situation after only 600 epochs whcih is cpmparably faster than the precious section. Usig Experience Replay Buffer clearlsy shows that how benifiting drom past expericenc can effectivly increase the convergance time of the algortihm.\n",
    "Advantages of Experience Replay Buffer:\n",
    "1- More efficient use of previous experience, by learning with it multiple times. This is key when gaining real-world experience is costly, you can get full use of it. The Q-learning updates are incremental and do not converge quickly, so multiple passes with the same data is beneficial, especially when there is low variance in immediate outcomes (reward, next state) given the same state, action pair. 3- Better convergence behaviour when training a function approximator.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[Episode   10/2000] [Steps   14] [reward 15.0]\n",
    "[Episode   20/2000] [Steps   11] [reward 12.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 25] [Average Reward 12.5]\n",
    "----------\n",
    "[Episode   30/2000] [Steps   12] [reward 13.0]\n",
    "[Episode   40/2000] [Steps   12] [reward 13.0]\n",
    "[Episode   50/2000] [Steps   14] [reward 15.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 50] [Average Reward 12.7]\n",
    "----------\n",
    "[Episode   60/2000] [Steps   11] [reward 12.0]\n",
    "[Episode   70/2000] [Steps   10] [reward 11.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 75] [Average Reward 12.9]\n",
    "----------\n",
    "[Episode   80/2000] [Steps   12] [reward 13.0]\n",
    "[Episode   90/2000] [Steps   12] [reward 13.0]\n",
    "[Episode  100/2000] [Steps   13] [reward 14.0]\n",
    "----------\n",
    "[TEST Episode 100] [Average Reward 11.6]\n",
    "----------\n",
    "[Episode  110/2000] [Steps   10] [reward 11.0]\n",
    "[Episode  120/2000] [Steps   11] [reward 12.0]\n",
    "----------\n",
    "[TEST Episode 125] [Average Reward 11.3]\n",
    "----------\n",
    "[Episode  130/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  140/2000] [Steps   10] [reward 11.0]\n",
    "[Episode  150/2000] [Steps   14] [reward 15.0]\n",
    "----------\n",
    "[TEST Episode 150] [Average Reward 12.9]\n",
    "----------\n",
    "[Episode  160/2000] [Steps    9] [reward 10.0]\n",
    "[Episode  170/2000] [Steps   15] [reward 16.0]\n",
    "----------\n",
    "[TEST Episode 175] [Average Reward 12.9]\n",
    "----------\n",
    "[Episode  180/2000] [Steps   10] [reward 11.0]\n",
    "[Episode  190/2000] [Steps   11] [reward 12.0]\n",
    "[Episode  200/2000] [Steps   14] [reward 15.0]\n",
    "----------\n",
    "[TEST Episode 200] [Average Reward 11.9]\n",
    "----------\n",
    "[Episode  210/2000] [Steps   10] [reward 11.0]\n",
    "[Episode  220/2000] [Steps   13] [reward 14.0]\n",
    "----------\n",
    "[TEST Episode 225] [Average Reward 12.1]\n",
    "----------\n",
    "[Episode  230/2000] [Steps   15] [reward 16.0]\n",
    "[Episode  240/2000] [Steps   14] [reward 15.0]\n",
    "[Episode  250/2000] [Steps   10] [reward 11.0]\n",
    "----------\n",
    "[TEST Episode 250] [Average Reward 12.4]\n",
    "----------\n",
    "[Episode  260/2000] [Steps    8] [reward 9.0]\n",
    "[Episode  270/2000] [Steps    8] [reward 9.0]\n",
    "----------\n",
    "[TEST Episode 275] [Average Reward 9.5]\n",
    "----------\n",
    "[Episode  280/2000] [Steps   11] [reward 12.0]\n",
    "[Episode  290/2000] [Steps   10] [reward 11.0]\n",
    "[Episode  300/2000] [Steps   10] [reward 11.0]\n",
    "----------\n",
    "[TEST Episode 300] [Average Reward 11.9]\n",
    "----------\n",
    "[Episode  310/2000] [Steps   23] [reward 24.0]\n",
    "[Episode  320/2000] [Steps  110] [reward 111.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 325] [Average Reward 147.9]\n",
    "----------\n",
    "[Episode  330/2000] [Steps  172] [reward 173.0]\n",
    "[Episode  340/2000] [Steps  132] [reward 133.0]\n",
    "[Episode  350/2000] [Steps  141] [reward 142.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 350] [Average Reward 157.5]\n",
    "----------\n",
    "[Episode  360/2000] [Steps  192] [reward 193.0]\n",
    "[Episode  370/2000] [Steps  144] [reward 145.0]\n",
    "----------\n",
    "[TEST Episode 375] [Average Reward 154.9]\n",
    "----------\n",
    "[Episode  380/2000] [Steps  130] [reward 131.0]\n",
    "[Episode  390/2000] [Steps  108] [reward 109.0]\n",
    "[Episode  400/2000] [Steps  157] [reward 158.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 400] [Average Reward 160.5]\n",
    "----------\n",
    "[Episode  410/2000] [Steps  169] [reward 170.0]\n",
    "[Episode  420/2000] [Steps  170] [reward 171.0]\n",
    "----------\n",
    "[TEST Episode 425] [Average Reward 143.5]\n",
    "----------\n",
    "[Episode  430/2000] [Steps  175] [reward 176.0]\n",
    "[Episode  440/2000] [Steps  138] [reward 139.0]\n",
    "[Episode  450/2000] [Steps  147] [reward 148.0]\n",
    "----------\n",
    "[TEST Episode 450] [Average Reward 141.3]\n",
    "----------\n",
    "[Episode  460/2000] [Steps  193] [reward 194.0]\n",
    "[Episode  470/2000] [Steps  142] [reward 143.0]\n",
    "----------\n",
    "[TEST Episode 475] [Average Reward 135.8]\n",
    "----------\n",
    "[Episode  480/2000] [Steps  150] [reward 151.0]\n",
    "[Episode  490/2000] [Steps  161] [reward 162.0]\n",
    "[Episode  500/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 500] [Average Reward 187.8]\n",
    "----------\n",
    "[Episode  510/2000] [Steps  169] [reward 170.0]\n",
    "[Episode  520/2000] [Steps  165] [reward 166.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 525] [Average Reward 191.6]\n",
    "----------\n",
    "[Episode  530/2000] [Steps  172] [reward 173.0]\n",
    "[Episode  540/2000] [Steps  126] [reward 127.0]\n",
    "[Episode  550/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "saving model.\n",
    "[TEST Episode 550] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  560/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  570/2000] [Steps  176] [reward 177.0]\n",
    "----------\n",
    "[TEST Episode 575] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  580/2000] [Steps  104] [reward 105.0]\n",
    "[Episode  590/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  600/2000] [Steps  118] [reward 119.0]\n",
    "----------\n",
    "[TEST Episode 600] [Average Reward 191.1]\n",
    "----------\n",
    "[Episode  610/2000] [Steps  102] [reward 103.0]\n",
    "[Episode  620/2000] [Steps  121] [reward 122.0]\n",
    "----------\n",
    "[TEST Episode 625] [Average Reward 148.1]\n",
    "----------\n",
    "[Episode  630/2000] [Steps  176] [reward 177.0]\n",
    "[Episode  640/2000] [Steps  189] [reward 190.0]\n",
    "[Episode  650/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 650] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  660/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  670/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 675] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  680/2000] [Steps  184] [reward 185.0]\n",
    "[Episode  690/2000] [Steps  160] [reward 161.0]\n",
    "[Episode  700/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 700] [Average Reward 198.9]\n",
    "----------\n",
    "[Episode  710/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  720/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 725] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  730/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  740/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  750/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 750] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  760/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  770/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 775] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  780/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  790/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  800/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 800] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  810/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  820/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 825] [Average Reward 153.7]\n",
    "----------\n",
    "[Episode  830/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  840/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  850/2000] [Steps  164] [reward 165.0]\n",
    "----------\n",
    "[TEST Episode 850] [Average Reward 192.6]\n",
    "----------\n",
    "[Episode  860/2000] [Steps  178] [reward 179.0]\n",
    "[Episode  870/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 875] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  880/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  890/2000] [Steps  199] [reward 200.0]\n",
    "[Episode  900/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 900] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode  910/2000] [Steps  194] [reward 195.0]\n",
    "[Episode  920/2000] [Steps  175] [reward 176.0]\n",
    "----------\n",
    "[TEST Episode 925] [Average Reward 162.3]\n",
    "----------\n",
    "[Episode  930/2000] [Steps  178] [reward 179.0]\n",
    "[Episode  940/2000] [Steps  171] [reward 172.0]\n",
    "[Episode  950/2000] [Steps  180] [reward 181.0]\n",
    "----------\n",
    "[TEST Episode 950] [Average Reward 141.6]\n",
    "----------\n",
    "[Episode  960/2000] [Steps  168] [reward 169.0]\n",
    "[Episode  970/2000] [Steps  193] [reward 194.0]\n",
    "----------\n",
    "[TEST Episode 975] [Average Reward 140.9]\n",
    "----------\n",
    "[Episode  980/2000] [Steps  170] [reward 171.0]\n",
    "[Episode  990/2000] [Steps  187] [reward 188.0]\n",
    "[Episode 1000/2000] [Steps  168] [reward 169.0]\n",
    "----------\n",
    "[TEST Episode 1000] [Average Reward 167.7]\n",
    "----------\n",
    "[Episode 1010/2000] [Steps  191] [reward 192.0]\n",
    "[Episode 1020/2000] [Steps  180] [reward 181.0]\n",
    "----------\n",
    "[TEST Episode 1025] [Average Reward 150.4]\n",
    "----------\n",
    "[Episode 1030/2000] [Steps  170] [reward 171.0]\n",
    "[Episode 1040/2000] [Steps  185] [reward 186.0]\n",
    "[Episode 1050/2000] [Steps  195] [reward 196.0]\n",
    "----------\n",
    "[TEST Episode 1050] [Average Reward 174.6]\n",
    "----------\n",
    "[Episode 1060/2000] [Steps  183] [reward 184.0]\n",
    "[Episode 1070/2000] [Steps  194] [reward 195.0]\n",
    "----------\n",
    "[TEST Episode 1075] [Average Reward 199.9]\n",
    "----------\n",
    "[Episode 1080/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1090/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1100/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1100] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1110/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1120/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1125] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1130/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1140/2000] [Steps  195] [reward 196.0]\n",
    "[Episode 1150/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1150] [Average Reward 199.8]\n",
    "----------\n",
    "[Episode 1160/2000] [Steps  194] [reward 195.0]\n",
    "[Episode 1170/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1175] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1180/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1190/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1200/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1200] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1210/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1220/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1225] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1230/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1240/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1250/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1250] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1260/2000] [Steps  195] [reward 196.0]\n",
    "[Episode 1270/2000] [Steps  187] [reward 188.0]\n",
    "----------\n",
    "[TEST Episode 1275] [Average Reward 193.7]\n",
    "----------\n",
    "[Episode 1280/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1290/2000] [Steps  174] [reward 175.0]\n",
    "[Episode 1300/2000] [Steps  181] [reward 182.0]\n",
    "----------\n",
    "[TEST Episode 1300] [Average Reward 181.2]\n",
    "----------\n",
    "[Episode 1310/2000] [Steps  193] [reward 194.0]\n",
    "[Episode 1320/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1325] [Average Reward 177.0]\n",
    "----------\n",
    "[Episode 1330/2000] [Steps  166] [reward 167.0]\n",
    "[Episode 1340/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1350/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1350] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1360/2000] [Steps  196] [reward 197.0]\n",
    "[Episode 1370/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1375] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1380/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1390/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1400/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1400] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1410/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1420/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1425] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1430/2000] [Steps  172] [reward 173.0]\n",
    "[Episode 1440/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1450/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1450] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1460/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1470/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1475] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1480/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1490/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1500/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1500] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1510/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1520/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1525] [Average Reward 197.6]\n",
    "----------\n",
    "[Episode 1530/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1540/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1550/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1550] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1560/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1570/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1575] [Average Reward 197.9]\n",
    "----------\n",
    "[Episode 1580/2000] [Steps  183] [reward 184.0]\n",
    "[Episode 1590/2000] [Steps  185] [reward 186.0]\n",
    "[Episode 1600/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1600] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1610/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1620/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1625] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1630/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1640/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1650/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1650] [Average Reward 199.4]\n",
    "----------\n",
    "[Episode 1660/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1670/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1675] [Average Reward 184.7]\n",
    "----------\n",
    "[Episode 1680/2000] [Steps  191] [reward 192.0]\n",
    "[Episode 1690/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1700/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1700] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1710/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1720/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1725] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1730/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1740/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1750/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1750] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1760/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1770/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1775] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1780/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1790/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1800/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1800] [Average Reward 191.8]\n",
    "----------\n",
    "[Episode 1810/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1820/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1825] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1830/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1840/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1850/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1850] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1860/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1870/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1875] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1880/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1890/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1900/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1900] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1910/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1920/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1925] [Average Reward 197.2]\n",
    "----------\n",
    "[Episode 1930/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1940/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1950/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1950] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1960/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1970/2000] [Steps  199] [reward 200.0]\n",
    "----------\n",
    "[TEST Episode 1975] [Average Reward 200.0]\n",
    "----------\n",
    "[Episode 1980/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 1990/2000] [Steps  199] [reward 200.0]\n",
    "[Episode 2000/2000] [Steps  189] [reward 190.0]\n",
    "----------\n",
    "[TEST Episode 2000] [Average Reward 198.9]\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Extra\n",
    "\n",
    "Ideas to experiment with:\n",
    "\n",
    "Is $\\epsilon$-greedy strategy the best strategy available? Why not trying something different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: As discussed earlier, we have used a decaying function in DQN Section to decrease epsilon after each espidoe and achieve faster convergence. The function that we have used is called Linear Annealing, and it is as follows:\n",
    "\n",
    "    epsilon = max(0.02, 0.1 - 0.01*(i_episode/200)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
